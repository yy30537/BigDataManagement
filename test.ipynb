{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, RDD\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split, col, size\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from statistics import pvariance\n",
    "\n",
    "def get_spark_context(on_server) -> SparkContext:\n",
    "    spark_conf = SparkConf().setAppName(\"2AMD15\")\n",
    "    if not on_server:\n",
    "        spark_conf = spark_conf.setMaster(\"local[*]\")\n",
    "    spark_context = SparkContext.getOrCreate(spark_conf)\n",
    "\n",
    "    if on_server:\n",
    "        # TODO: You may want to change ERROR to WARN to receive more info. For larger data sets, to not set the\n",
    "        # log level to anything below WARN, Spark will print too much information.\n",
    "        spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark_context\n",
    "\n",
    "on_server = False  # TODO: Set this to true if and only if deploying to the server\n",
    "\n",
    "spark_context = get_spark_context(on_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| key|               value|\n",
      "+----+--------------------+\n",
      "|QH9W| [6, 17, 66, 66, 39]|\n",
      "|YRBC|[16, 60, 34, 35, 32]|\n",
      "|CDL3|[23, 87, 61, 62, 43]|\n",
      "|X1E8|[25, 50, 29, 58, 34]|\n",
      "|P1WH|[26, 39, 34, 38, 41]|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def q1a(spark_context: SparkContext, on_server: bool) -> DataFrame:\n",
    "    vectors_file_path = \"/vectors.csv\" if on_server else \"./vectors.csv\"\n",
    "\n",
    "    spark = SparkSession(spark_context) \n",
    "\n",
    "    # TODO: Implement Q1a here by creating a Dataset of DataFrame out of the file at {@code vectors_file_path}.\n",
    "\n",
    "    # Read CSV file into DataFrame\n",
    "    df = spark.read.option(\"header\", \"false\") \\\n",
    "        .csv(vectors_file_path) \\\n",
    "        .withColumnRenamed(\"_c0\", \"key\").withColumnRenamed(\"_c1\", \"value\") \\\n",
    "        .select('key', split(col(\"value\"),\";\").cast(\"array<int>\").alias(\"value\")) \\\n",
    "        .sort('value')\n",
    "\n",
    "\n",
    "    # df.sort('value').explain() # explain(): 显示当前任务的lineage\n",
    "\n",
    "    df.show(n=5)\n",
    "    # df.take(6) # take 拿出来的是array\n",
    "    \n",
    "\n",
    "    # df_size = df.select(size(\"value\").alias(\"vector length\")) # size(): return the vector length\n",
    "    # df_size.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = q1a(spark_context, on_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 33, 29, 27, 37]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str2vector(cell:str) -> list:\n",
    "    ''' \n",
    "    Turn a dataframe cell that is a str form of a vector into a numpy vector / or a list\n",
    "    '''\n",
    "    return [int(num) for num in cell.split(';')]\n",
    "\n",
    "cell = df.collect()[0][1]\n",
    "str2vector(cell=cell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1b\n",
    "def q1b(spark_context: SparkContext, on_server: bool) -> RDD:\n",
    "    vectors_file_path = \"/vectors.csv\" if on_server else \"./vectors.csv\"\n",
    "\n",
    "    # TODO: Implement Q1b here by creating an RDD out of the file at {@code vectors_file_path}.\n",
    "\n",
    "    vectors_rdd01 = spark_context.textFile(vectors_file_path)\n",
    "\n",
    "    return rdd \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./vectors.csv MapPartitionsRDD[372] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_file_path = \"/vectors.csv\" if on_server else \"./vectors.csv\"\n",
    "rdd = spark_context.textFile(vectors_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define UDF\n",
    "\n",
    "compute_var = udf(lambda vector: pvariance(vector), FloatType())\n",
    "aggregate = udf(lambda x, y, z: pvariance(\n",
    "    [x[i] + y[i] + z[i]\n",
    "    for i in range( len(x) ) ]\n",
    "))\n",
    "\n",
    "# df.select(\n",
    "#     compute_var(col('value')).alias('variance')\n",
    "# ).show(10)\n",
    "\n",
    "sqlWay.select(\n",
    "    aggregate(col('v1'),col('v2'),col('v3')).alias('var')\n",
    ").count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 15:39:11 WARN SimpleFunctionRegistry: The function compute_var replaced a previously registered function.\n",
      "23/03/08 15:39:11 WARN SimpleFunctionRegistry: The function aggregate_var replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, y, z)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use UDF in SQL\n",
    "\n",
    "spark.udf.register(\"compute_var\", compute_var) # register in SQL\n",
    "spark.udf.register('aggregate_var', aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'string'), ('value', 'string')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows 2689\n",
      "+----+----+----+------------------+\n",
      "| id1| id2| id3|               var|\n",
      "+----+----+----+------------------+\n",
      "|K573|QEL9|FH4I|            331.76|\n",
      "|K573|ZQ5O|UJXN|            256.56|\n",
      "|K573|ZQ5O|PW9U|            283.76|\n",
      "|K573|EEO5|ZQ5O|213.35999999999999|\n",
      "|K573|EEO5|RJJ7|            395.36|\n",
      "|K573|OE4G|ZQ5O|             212.4|\n",
      "|K573|OE4G|FH4I|            265.04|\n",
      "|K573|OE4G|RJJ7|              73.6|\n",
      "|K573|OE4G|BX50|387.84000000000003|\n",
      "|K573|OE4G|WAPJ|             88.64|\n",
      "|K573|OE4G|JU8G|228.64000000000001|\n",
      "|K573|FH4I|ZQ5O|            400.24|\n",
      "|K573|XLCC|ZQ5O|328.96000000000004|\n",
      "|K573|XLCC|RJJ7|            298.16|\n",
      "|K573|RJJ7|ZQ5O|             335.6|\n",
      "|K573|RJJ7|FH4I|            373.84|\n",
      "|K573|RJJ7|UJXN|            216.96|\n",
      "|K573|RJJ7|BX50|            255.84|\n",
      "|K573|RJJ7|PW9U|229.76000000000002|\n",
      "|K573|BX50|ZQ5O|            263.44|\n",
      "+----+----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"vectors\")\n",
    "spark = SparkSession(spark_context) \n",
    "\n",
    "# aggregate_var(c1,c2,c3) as var\n",
    "\n",
    "tau = 410\n",
    "\n",
    "sqlWay = spark.sql('''\n",
    "SELECT id1, id2, id3, var \n",
    "FROM (\n",
    "    SELECT id1, id2, id3, aggregate_var(v1,v2,v3) as var\n",
    "    FROM(\n",
    "        SELECT  vectors1.key as id1, vectors2.key as id2, vectors3.key as id3, vectors1.value as v1, vectors2.value as v2, vectors3.value as v3\n",
    "        FROM vectors as vectors1, vectors as vectors2, vectors as vectors3  \n",
    "        WHERE vectors1.value < vectors2.value and vectors2.value < vectors3.value\n",
    "    )\n",
    ")\n",
    "WHERE var < 410\n",
    "''')\n",
    "\n",
    "# sqlWay = spark.sql('''\n",
    "# SELECT vectors1.value as v1, vectors2.value as v2, vectors3.value as v3\n",
    "# FROM vectors as vectors1, vectors as vectors2, vectors as vectors3  \n",
    "# WHERE vectors1.value < vectors2.value and vectors2.value < vectors3.value\n",
    "# ''')\n",
    "\n",
    "\n",
    "print('number of rows ' + str(sqlWay.count()))\n",
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2(spark_context: SparkContext, data_frame: DataFrame):\n",
    "    # TODO: Implement Q2 here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2(spark_context, data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "| key|         value|\n",
      "+----+--------------+\n",
      "|K573|44;33;29;27;37|\n",
      "|ZNSR|37;39;15;37;42|\n",
      "|CDL3|23;87;61;62;43|\n",
      "|PQ9E|29;40;46;13;33|\n",
      "|QWOS|37;24;51;33;43|\n",
      "|HJ0O| 36;2;68;36;44|\n",
      "|M46E|41;41;29;78;53|\n",
      "|X1E8|25;50;29;58;34|\n",
      "|LNZN|27;17;38;26;39|\n",
      "|V1N7|39;58;63;57;23|\n",
      "|QEL9|61;84;10;75;52|\n",
      "|ZQ5O|62;45;47;47;79|\n",
      "|ZD9B|  77;97;7;20;1|\n",
      "|ZAXW|86;80;22;74;27|\n",
      "|EEO5| 53;42;59;50;5|\n",
      "|XCXR|27;65;16;50;34|\n",
      "|OE4G|47;54;37;54;33|\n",
      "|FH4I|62;37;80;51;37|\n",
      "|YRBC|16;60;34;35;32|\n",
      "|LU0Q|91;81;41;40;99|\n",
      "+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "on_server = False  # TODO: Set this to true if and only if deploying to the server\n",
    "\n",
    "spark_context = get_spark_context(on_server)\n",
    "\n",
    "data_frame = q1a(spark_context, on_server)\n",
    "\n",
    "data_frame.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef0ca4ef26fab86c1810dfc6a810164de3b439540530453dc74693bc65bf42c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
